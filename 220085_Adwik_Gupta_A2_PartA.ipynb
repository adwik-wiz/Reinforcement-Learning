{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "**Name**:  Adwik Gupta                             </br>\n",
    "**Roll No.**: 220085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "    '''\n",
    "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
    "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        '''\n",
    "        Constructor for the StochasticMazeEnv class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 12\n",
    "            no_actions : The no. of possible actions which is 4\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA-1)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StochasticMazeEnv()\n",
    "env.reset()\n",
    "num_states = env.nS\n",
    "num_actions = env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases for checking the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   3 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 10\n",
      "Final Reward: 0.91\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(rand_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Right Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 3\n",
      "Final Reward: 0.98\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01, -0.01, -0.01, 1, -0.01, -0.01, -1, -0.01, -0.01, -0.01, -0.01]\n",
      "[0, 3, 3, 1, 0, 2, 3, 2, 3, 3, 1]\n",
      "value ex [-0.030000000000000006, -0.030000000000000006, 0.17200000000000001, 2.0, -0.030000000000000006, -1.614, -2.0, -0.030000000000000006, -0.030000000000000006, -0.030000000000000006, -1.614]\n",
      "value ex [-0.07000000000000002, 0.09160000000000001, 2.4258000000000006, 4.0, -0.07000000000000002, -1.9468000000000003, -4.0, -0.07000000000000002, -0.07000000000000002, -0.22840000000000005, -2.1084]\n",
      "value ex [-0.020720000000000006, 2.040560000000001, 6.4717, 8.0, -0.15000000000000005, -0.7098399999999996, -8.0, -0.15000000000000005, -0.15000000000000005, -0.51192, -3.0109600000000003]\n",
      "value ex [1.584656000000001, 7.616032000000002, 14.245886, 16.0, -0.2065760000000001, 3.4875360000000017, -16.0, -0.31000000000000016, -0.31000000000000016, -0.764096, -4.630592]\n",
      "value ex [7.805289600000003, 20.525947200000004, 29.6172282, 32.0, 1.0098336000000008, 13.523998400000004, -32.0, -0.5472608000000003, -0.6300000000000003, 1.521873600000002, -7.413928]\n",
      "value ex [25.097559680000003, 48.3149192, 60.329350860000005, 64.0, 7.446032000000004, 35.261180800000005, -64.0, 0.13288000000000022, 0.45149888000000127, 11.526679520000007, -10.246821919999999]\n",
      "[2, 2, 2, 1, 1, 1, 3, 1, 2, 1, 0]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# setting the default values of V  of all states\n",
    "\n",
    "V = [-0.01,-0.01,-0.01,1,-0.01,-0.01,-1,-0.01,-0.01,-0.01,-0.01]\n",
    "print(V)\n",
    "\n",
    "# setting the default policy\n",
    "\n",
    "pi = [env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action() ,env.sample_action()]\n",
    "pi_copy = pi.copy()\n",
    "#pi = [2]*11\n",
    "print(pi)\n",
    "\n",
    "# hint: code policy evaluation function\n",
    "\n",
    "def policy_eval(value,policy):\n",
    "    states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "    val = value.copy()\n",
    "    for j in range(11):\n",
    "        action_set = env.P()[states[j]][policy[j]]\n",
    "        for action in action_set:\n",
    "            if(action[1]<5):\n",
    "                value[j] += action[0]*(action[2]+val[action[1]])\n",
    "            else:\n",
    "                value[j] += action[0]*(action[2]+val[action[1]-1])               \n",
    "    print(\"value ex\",value)            \n",
    "    return value\n",
    "\n",
    "# hint: code policy improvement function\n",
    "\n",
    "def policy_improvement(value,policy):\n",
    "    states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "    for j in states:\n",
    "        v = [0]*4\n",
    "        path_set = env.P()[j]\n",
    "        #for i,path in enumerate(path_set):\n",
    "        for i in range(4):\n",
    "            for path_in in path_set[i]:\n",
    "                if path_in[1]<5:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]])\n",
    "                else:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]-1])\n",
    "        #print(j,v)            \n",
    "        if len(set(v)) != 1:\n",
    "            max_index = v.index(max(v))\n",
    "            if j<5:\n",
    "                policy[j] = max_index\n",
    "            else:\n",
    "                policy[j-1] = max_index  \n",
    "        #print(\"policy\",policy)             \n",
    "    return policy         \n",
    "\n",
    "# hint: implement policy iteration\n",
    "count = 0\n",
    "\n",
    "while(1):\n",
    "    count += 1\n",
    "    pi_dummy = pi.copy()\n",
    "    V = policy_eval(V,pi)\n",
    "    pi = policy_improvement(V,pi)\n",
    "    if pi_dummy == pi:\n",
    "        break\n",
    "\n",
    "print(pi)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 3\n",
      "Final Reward: 0.98\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "state_action = [[states[i], pi[i]] for i in range(11)]\n",
    "state = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "    \n",
    "    right_action = state_action[state][1] if state<5 else state_action[state-1][1]   #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.020000000000000004, -0.020000000000000004, 1.596, 1.0, -0.020000000000000004, 1.0668000000000002, -1.0, -0.020000000000000004, -0.020000000000000004, 0.8404400000000002, 0.46235200000000015]\n",
      "[-0.030000000000000013, 1.2628000000000004, 1.8642800000000002, 1.0, -0.030000000000000013, 1.3891040000000003, -1.0, -0.030000000000000013, 0.6583520000000002, 1.2133536000000005, 0.8079180800000005]\n",
      "[0.9942400000000002, 1.7339840000000004, 1.9233384, 1.0, 0.7793920000000002, 1.46858112, -1.0, 0.6763488000000002, 1.0923532800000006, 1.3548920320000002, 0.9557054336000003]\n",
      "[1.5545504000000006, 1.8754675200000002, 1.937191952, 1.0, 1.3895187200000005, 1.4940732944000001, -1.0, 1.2784851840000007, 1.2923842816000004, 1.4100676070400002, 1.0146246289920002]\n",
      "[1.7847809280000004, 1.9148470656, 1.94112652464, 1.0, 1.6957284864000006, 1.520378048688, -1.0, 1.6036697356800007, 1.5314126448640006, 1.508174681464001, 1.099002208070401]\n",
      "[1.8699285939200003, 1.9258706328319999, 1.9441504573328001, 1.0, 1.8250885724160004, 1.5515349528300804, -1.0, 1.7635790959872004, 1.7071458057625608, 1.661687608039457, 1.230250307238606]\n",
      "[1.9001982228992, 1.9304944924326402, 1.9475685410162882, 1.0, 1.8751762928025602, 1.592153577169639, -1.0, 1.8372135244170245, 1.8011999806861319, 1.7563441030698153, 1.319100313179713]\n",
      "[1.9119330455162882, 1.9341537312995587, 1.9519722118185927, 1.0, 1.8945816949735426, 1.634554493224552, -1.0, 1.8695067064891497, 1.8458453613285462, 1.8057661486922738, 1.3675229502717903]\n",
      "[1.9179744590886303, 1.938408515714786, 1.9566526705043146, 1.0, 1.9032959062656127, 1.6738854764993003, -1.0, 1.8841719317942598, 1.8665066177011171, 1.8311704566800513, 1.4038877009126165]\n",
      "[1.922853849107253, 1.9430038395464089, 1.9610538147003616, 1.0, 1.9089422605389252, 1.7083308083374815, -1.0, 1.8922216633806779, 1.8770786542447657, 1.8456130498975662, 1.4380602358111116]\n",
      "[1.9275826826017448, 1.9474438196695711, 1.9649384623037842, 1.0, 1.913854598189181, 1.7377197978901202, -1.0, 1.8980137103138892, 1.883826699100065, 1.855394644058821, 1.4697936766358826]\n",
      "[1.9320987838147494, 1.9514395337769417, 1.9682658260193904, 1.0, 1.9184499466896359, 1.7625418853199175, -1.0, 1.9029439982931042, 1.8891205384544965, 1.8630900837014712, 1.4991233173424416]\n",
      "[1.9362065000719921, 1.9549005675709008, 1.9710807711339309, 1.0, 1.922655189395521, 1.7834505937394745, -1.0, 1.9073306051911771, 1.8936885918438413, 1.8696049412191678, 1.5261714797301145]\n",
      "[1.9398066230034718, 1.957844730421325, 1.9734531364873407, 1.0, 1.9263763362818818, 1.8010662827622306, -1.0, 1.9112029887290074, 1.8977001093519743, 1.8752272098797194, 1.5510770527450752]\n",
      "[1.9428940802655954, 1.9603314552741375, 1.9754519419249572, 1.0, 1.9295905314688528, 1.8159209413902522, -1.0, 1.9145627349831806, 1.9011902098569395, 1.880066983012549, 1.5739760457718226]\n",
      "[1.945513625392755, 1.9624278445947931, 1.9771372883315212, 1.0, 1.9323290066079748, 1.8284571802466087, -1.0, 1.917438499770392, 1.9041888417877013, 1.8842034897560769, 1.594998790170248]\n",
      "[1.9477265388759075, 1.9641953995841757, 1.9785594468578132, 1.0, 1.934647032422321, 1.839042037858676, -1.0, 1.919880360093666, 1.906742056432473, 1.887718197907454, 1.6142707309439688]\n",
      "[1.9495936767971636, 1.9656866374030857, 1.9797601484716492, 1.0, 1.9366043479221953, 1.8479814649248512, -1.0, 1.92194571999037, 1.9089049872787909, 1.8906939561062635, 1.6319130534601982]\n",
      "[1.9511691123944046, 1.9669454462579368, 1.9807741613396501, 1.0, 1.938256159499963, 1.8555319836844724, -1.0, 1.9236899983268865, 1.9107329961172674, 1.8932089908728875, 1.6480426472014673]\n",
      "[1.9524988841957864, 1.9680084183233073, 1.9816306145024123, 1.0, 1.939650339256622, 1.8619095474851082, -1.0, 1.925162570849713, 1.912276655903224, 1.8953331785583791, 1.6627717003371587]\n",
      "[1.953621657003887, 1.9689061752665913, 1.9823540161987523, 1.0, 1.940827393454434, 1.8672963574637997, -1.0, 1.9264058374388409, 1.9135800011317174, 1.897126954507592, 1.6762072257542022]\n",
      "[1.9545698452591052, 1.96966444801232, 1.9829650373662553, 1.0, 1.9418213548981713, 1.8718462851584248, -1.0, 1.927455667775593, 1.9146805344468179, 1.8986417515240561, 1.6884506783311877]\n",
      "[1.9553706784255838, 1.9703049194954683, 1.983481132252468, 1.0, 1.9426608137201014, 1.8756893165043924, -1.0, 1.9283422711983225, 1.9156099238480218, 1.8999210458812623, 1.6995977150861952]\n",
      "[1.9560470848109432, 1.970845889701068, 1.9839170448756862, 1.0, 1.9433698305927751, 1.8789352622792088, -1.0, 1.9290910839788546, 1.916394851952688, 1.9010015123781978, 1.7097380948153955]\n",
      "[1.9566184033012264, 1.9713028138407627, 1.9842852307154897, 1.0, 1.9439686887595362, 1.8816768841327358, -1.0, 1.9297235446007834, 1.9170578060711645, 1.9019140845080251, 1.7189556937846584]\n",
      "[1.9566184033012264, 1.9713028138407627, 1.9842852307154897, 1.0, 1.9439686887595362, 1.8816768841327358, -1.0, 1.9297235446007834, 1.9170578060711645, 1.9019140845080251, 1.7189556937846584]\n",
      "25\n",
      "pi [2, 2, 2, 1, 1, 0, 3, 1, 0, 0, 3]\n",
      "[2, 2, 2, 1, 1, 0, 3, 1, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "V1 = [-0.01,-0.01,-0.01,1,-0.01,-0.01,-1,-0.01,-0.01,-0.01,-0.01]\n",
    "def value_iteration(value):\n",
    "    states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "    for j in states:\n",
    "        v = [0]*4\n",
    "        path_set = env.P()[j]\n",
    "        for i in range(4):\n",
    "            for path_in in path_set[i]:\n",
    "                if path_in[1]<5:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]])\n",
    "                else:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]-1])\n",
    "        if j<5:\n",
    "            value[j] = max(v)\n",
    "        else:\n",
    "            value[j-1] = max(v)\n",
    "    print(value)\n",
    "    return value\n",
    "c = 0\n",
    "while(1):\n",
    "    c += 1\n",
    "    V2 = V1.copy()\n",
    "    V1 = value_iteration(V1)\n",
    "    if all(abs(v2_i - v1_i) < 0.01 for v2_i, v1_i in zip(V2, V1)):\n",
    "        break\n",
    "print(V1)    \n",
    "print(c)\n",
    "\n",
    "def policy_finder(value,policy):\n",
    "    states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "    for j in states:\n",
    "        v = [0]*4\n",
    "        path_set = env.P()[j]\n",
    "        #for i,path in enumerate(path_set):\n",
    "        for i in range(4):\n",
    "            for path_in in path_set[i]:\n",
    "                if path_in[1]<5:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]])\n",
    "                else:\n",
    "                    v[i] += path_in[0]*(path_in[2]+value[path_in[1]-1])\n",
    "        #print(j,v)            \n",
    "        if len(set(v)) != 1:\n",
    "            max_index = v.index(max(v))\n",
    "            if j<5:\n",
    "                policy[j] = max_index\n",
    "            else:\n",
    "                policy[j-1] = max_index  \n",
    "        #print(\"policy\",policy)             \n",
    "    return policy    \n",
    "\n",
    "print(\"pi\",pi_copy)\n",
    "pi_vi = policy_finder(V1,pi_copy)\n",
    "print(pi_vi) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 3\n",
      "Final Reward: 0.98\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "states = [0,1,2,3,4,6,7,8,9,10,11]\n",
    "state_action = [[states[i], pi_vi[i]] for i in range(11)]\n",
    "state = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "    \n",
    "    right_action = state_action[state][1] if state<5 else state_action[state-1][1]   #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "In this case the policy obtained is not entirely same but both are leading to optimal solutions.\n",
    "VI may take more iterations, but each iteration tends to be less computationally expensive, as it performs both evaluation and improvement simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
